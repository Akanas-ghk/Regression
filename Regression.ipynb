{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. What is Simple Linear Regression?**\n",
        "Simple Linear Regression is a technique that models the relationship between two variables: one independent (X) and one dependent (Y). It fits a straight line (Y = mX + c) to predict Y from X. The method minimizes the sum of squared differences between observed and predicted Y values, making it useful for forecasting and identifying trends in linear relationships.\n",
        "\n",
        "# **Q2. What are the key assumptions of Simple Linear Regression?**\n",
        "The key assumptions are: (1) Linearity between X and Y, (2) Independence of residuals, (3) Homoscedasticity (constant variance of errors), (4) Normal distribution of errors, and (5) No or minimal multicollinearity. Violating these assumptions can lead to biased estimates or incorrect conclusions, so they must be validated through diagnostic plots and tests.\n",
        "\n",
        "# **Q3. What does the coefficient m represent in the equation Y = mX + c?**\n",
        "In the equation Y = mX + c, the coefficient m represents the slope of the regression line. It shows the change in the dependent variable (Y) for a one-unit change in the independent variable (X). A positive m means Y increases with X, while a negative m means Y decreases as X increases.\n",
        "\n",
        "#**Q4. What does the intercept c represent in the equation Y = mX + c?**\n",
        "The intercept c is the value of Y when X equals zero. It represents the point where the regression line crosses the Y-axis. It helps understand the baseline level of the dependent variable when the independent variable is absent or zero. Though sometimes not meaningful, it's necessary for the equation.\n",
        "\n",
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "The slope m is calculated using the formula:\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " )\n",
        "​\n",
        "\n",
        "This formula computes how much Y changes per unit change in X by minimizing the squared errors between actual and predicted values using the least squares approach.\n",
        "\n",
        "#**Q6. What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "The least squares method minimizes the sum of squared differences between the observed and predicted values of the dependent variable. It finds the best-fitting line (Y = mX + c) by minimizing error, making predictions as accurate as possible. It's the most common technique for estimating regression coefficients.\n",
        "\n",
        "#**Q7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "R² measures the proportion of variance in the dependent variable explained by the independent variable. R² = 1 means perfect prediction; R² = 0 means no predictive power. For example, an R² of 0.80 implies that 80% of the variation in Y is explained by X. Higher R² indicates better model fit.\n",
        "\n",
        "#**Q8. What is Multiple Linear Regression?**\n",
        "Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables. It extends simple linear regression by using the equation Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ. It helps to understand how multiple factors influence a single outcome and is widely used in real-world data analysis.\n",
        "\n",
        "#**Q9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "The main difference is the number of independent variables. Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more. Multiple regression allows for more complex models and can capture the combined effect of multiple factors on the dependent variable, leading to more accurate predictions.\n",
        "#**Q10. What are the key assumptions of Multiple Linear Regression?**\n",
        "Key assumptions include: (1) Linearity between predictors and response, (2) Independence of errors, (3) Homoscedasticity, (4) No multicollinearity among independent variables, (5) Normality of residuals. Violating these assumptions can result in unreliable estimates, so it's essential to diagnose and address issues using statistical tests and plots.\n",
        "\n",
        "#**Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "Heteroscedasticity occurs when the variance of errors is not constant across all levels of the independent variables. It violates a key regression assumption, potentially leading to biased standard errors, which affect hypothesis tests and confidence intervals. It makes predictions less reliable, so correcting it is vital using transformations or robust standard errors.\n",
        "\n",
        "#**Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "To improve a model with multicollinearity: (1) Remove highly correlated variables, (2) Use Principal Component Analysis (PCA), (3) Apply Ridge or Lasso regression, or (4) Combine correlated features. Multicollinearity inflates standard errors and makes coefficient interpretation unreliable, so reducing it enhances model stability and interpretability.\n",
        "\n",
        "#**Q13. What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "Common techniques include One-Hot Encoding (creating binary columns for each category), Label Encoding (assigning numeric codes), and Ordinal Encoding (for ordered categories). These transformations convert categorical data into numerical format, enabling them to be used in regression models. One-Hot is preferred for nominal data to avoid implying ordinal relationships.\n",
        "\n",
        "#**Q14. What is the role of interaction terms in Multiple Linear Regression?**\n",
        "Interaction terms capture the combined effect of two or more variables on the dependent variable. They help model scenarios where the effect of one variable depends on another. For example, the effect of experience on salary might vary by education level. Including interaction terms improves model accuracy and interpretation.\n",
        "\n",
        "#**Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "In Simple Linear Regression, the intercept represents the expected value of Y when X = 0. In Multiple Linear Regression, it represents Y when all independent variables equal zero. However, zero may not be meaningful for all variables, so interpretation depends on the context and variable scaling.\n",
        "\n",
        "#**Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "The slope indicates the rate of change in the dependent variable for a unit change in the independent variable. A positive slope shows a direct relationship, while a negative slope indicates an inverse one. Accurate slope estimation is critical for making reliable predictions and understanding variable influence.\n",
        "\n",
        "#**Q17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "The intercept provides the baseline value of the dependent variable when all independent variables are zero. It anchors the regression line and is essential for prediction. Though sometimes not meaningful practically, it's mathematically necessary and helps interpret the effect of predictors from a fixed starting point.\n",
        "\n",
        "#**Q18. What are the limitations of using R² as a sole measure of model performance?**\n",
        "R² only indicates explained variance, not model correctness or overfitting. It doesn't account for the number of predictors (unlike adjusted R²) or show if predictors are significant. A high R² can be misleading in poor models. It must be used alongside residual plots, p-values, and validation metrics like RMSE.\n",
        "\n",
        "#**Q19. How would you interpret a large standard error for a regression coefficient?**\n",
        "A large standard error suggests the coefficient estimate is uncertain and may not significantly differ from zero. It can result from multicollinearity, small sample size, or high data variability. Large standard errors reduce confidence in the variable's effect and widen confidence intervals, affecting model reliability.\n",
        "\n",
        "#**Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "Heteroscedasticity appears as a fan or funnel shape in residual vs. fitted value plots. Residuals spread unevenly instead of forming a constant band. It violates regression assumptions and leads to incorrect standard errors, impacting inference. Addressing it improves model accuracy and reliability, often via transformations or robust errors.\n",
        "\n",
        "#**Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "This indicates that adding predictors increased R² without genuinely improving the model. Adjusted R² penalizes non-significant predictors. A large gap suggests overfitting and inclusion of irrelevant variables. The model may seem good but lacks predictive power. Adjusted R² provides a better assessment of model usefulness.\n",
        "\n",
        "#**Q22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "Scaling ensures all variables contribute equally, especially when units differ. It improves numerical stability, speeds up convergence in gradient-based algorithms, and allows fair coefficient comparison. Without scaling, large-valued features might dominate the model, affecting interpretation and performance. Common techniques include Min-Max Scaling and Standardization (Z-score).\n",
        "\n",
        "#**Q23. What is polynomial regression?**\n",
        "Polynomial regression extends linear regression by fitting a polynomial equation (e.g., Y = b₀ + b₁X + b₂X² + ... + bₙXⁿ). It models nonlinear relationships between variables while still being linear in coefficients. It’s useful when data trends show curvature that simple linear models cannot capture.\n",
        "#**Q24. How does polynomial regression differ from linear regression?**\n",
        "Linear regression fits a straight line, assuming a linear relationship. Polynomial regression fits a curved line, using higher-degree terms of the independent variable. Though still linear in coefficients, it captures non-linear trends in data. It's more flexible but can overfit if the polynomial degree is too high.\n",
        "\n",
        "#**Q25. When is polynomial regression used?**\n",
        "Polynomial regression is used when data shows a curved trend that linear regression can't fit well. It’s helpful in capturing nonlinear patterns such as growth curves, seasonal variations, and complex relationships in scientific and engineering data. However, it should be used with caution to avoid overfitting.\n",
        "\n",
        "#**Q26. What is the general equation for polynomial regression?**\n",
        "The general form is:\n",
        "Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ\n",
        "Where b₀ is the intercept, b₁...bₙ are coefficients, and X is the independent variable. The degree n defines the curve’s complexity. It fits nonlinear data while maintaining linearity in parameters.\n",
        "\n",
        "#**Q27. Can polynomial regression be applied to multiple variables?**\n",
        "Yes, it can be extended to multiple variables. This involves using combinations of predictors raised to various powers and their interactions. For example, terms like X₁², X₂², or X₁X₂. It captures complex relationships but increases dimensionality and risk of overfitting, so regularization techniques may be needed.\n",
        "\n",
        "#**Q28. What are the limitations of polynomial regression?**\n",
        "Limitations include: (1) Overfitting if the degree is too high, (2) Poor extrapolation outside the data range, (3) Increasing complexity with more terms, (4) Sensitive to outliers. It may also suffer from multicollinearity due to high powers. Careful degree selection and validation are crucial.\n",
        "\n",
        "#**Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "Methods include: (1) Cross-validation, (2) Adjusted R², (3) AIC/BIC (penalized criteria), (4) Residual analysis, (5) RMSE or MAE. These help balance accuracy and complexity, avoiding overfitting. Visualization of the fitted curve versus actual data also aids in selecting the appropriate degree.\n",
        "\n",
        "#**Q30. Why is visualization important in polynomial regression?**\n",
        "Visualization helps assess how well the polynomial model fits data, identifies overfitting/underfitting, and ensures the curve captures the correct trend. It makes model interpretation easier, especially when relationships are nonlinear. Plots like scatter plots with fitted curves and residual plots are useful in understanding model performance.\n",
        "#**Q31. How is polynomial regression implemented in Python?**\n",
        "In Python, use PolynomialFeatures from sklearn.preprocessing to generate polynomial terms, then apply LinearRegression from sklearn.linear_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgqiV19z9iEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n"
      ],
      "metadata": {
        "id": "aZ_xGxjIMQpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}